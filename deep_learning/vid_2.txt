Once you created the filter, we applied it to each part of the image, and mapped the output to an output array.  This gave us a map showing where the associated pattern shows up in the image.[j]


Different convolutions or filters capture different aspect of the original image. In practice, you won’t manually pick the numbers in your filters. The numbers are set during model training processes called gradient descent and back-propagation, which we’ll come back to later.


But using those processes to automatically create filters means we can have a lot of filters, to capture many different patterns. Here we see a handwritten digit. Below that is its representation as a matrix, or a 2D tensor. Each convolution we apply to that tensor creates a new 2D tensor. We stack all those 2D tensors into a single 3D tensor.


For example, we might have a 2D tensor showing where the horizontal lines, and we stack it on top of the 2D tensor showing the convolution detecting vertical lines, and keep stacking with any other convolutions we have.  The result is a representation of the image in 3 dimensions.  [k]


Moving across the first dimension means moving horizontally through the image[l]
Moving across the second dimension means moving vertically through the image[m]
Moving through the last dimension takes you from the output of one convolution to the next. This last dimension is called the “channel” dimension.[n]


Now here is where things get pretty amazing.


We’re going to apply another set or “layer” convolutions. But we won’t apply them to the raw pixel intensities, like we did with the first set or layer of convolutions. Instead, we apply this layer of convolutions to that 3D tensor we got as the output of our first set of convolutions.


Again, each set of convolutions that get applied at the same time is called a layer.  So our first layer of convolutions took the raw pixel intensities and translated them into a 3D tensor indicating where the horizontal lines are, where the vertical lines are, where the dark spots are, and so on.


Our second layer of convolutions takes that map of pattern locations as input, and multiplies them with some 3D convolutions, to find more interesting patterns.


Just as patterns of dark or light pixels combine to make lines of different orientations, patterns between the lines combine to make more interesting shapes. For example, a box is just a pattern of horizontal and vertical lines in a certain spatial relationship to each other. So, the one convolution might take a tensor of line locations as input and produce a tensor of box locations.


The output of the second layer can feed into a third layer, which detects even more abstract or complex patterns.  And each time we apply a new layer of convolutions, we can detect even more interesting patterns.


For example, if you get the right type concentric circles at one layer, the subsequent layer might detect a wheel. Wheels in the right locations might help us detect a car.


Modern networks apply a lot of these layers. Some research papers have used over a thousand layers.


After enough layers, and the right numbers in the convolutions or filters, we have a 3D tensor with a very rich summary of the image content. What can you do with that summary of what objects are in what parts of the image?[o]


There are a lot of cool applications. We start with something called object detection.  Your model will take a photo as input, and it returns a prediction for what type of object is in the photo.  Is it a dog, or a kangaroo, or a robot, or whatever.


Many breakthroughs in computer vision and AI have come through an object detection competition called ImageNet.  Competitors receive a training set with millions of images. Each image is labeled with it’s content, coming from 1000 different possible labels.


Competitors use this training data to build a model that classifies images into those 1000 options.  The best models get it right with their top prediction about 80% of the time.  If each model makes 5 guesses, the true image label is in those top 5 guesses about 95% of the time.  This is especially impressive, because there are many similar options the model must choose choose between.[p]


So labeling an image correctly requires not only that you know it’s a picture of a shark, but you have to know that it is a hammerhead shark, or a tiger shark or a great white shark.  So, when those best models are right on their first guess 80% of the time, that’s better than most people could do.


Incidentally, data used in the ImageNet competitions are available on Kaggle.


Some models used in the ImageNet competitions are available as “pre-trained” models.  So even before you learn to train a model for yourself, you can start with powerful models that classify images into one of a thousand categories.


As a very first step to programming with TensorFlow, we’ll start by using a pre-trained model from the ImageNet competition.


Once you can run these models, we’ll move on to something called transfer learning, which allows you to apply this same level of power to most other problems you might be interested in.


I think transfer learning is mind-blowingly cool. But it starts by using these pre-trained models. So we’ll get up to speed with those first.
