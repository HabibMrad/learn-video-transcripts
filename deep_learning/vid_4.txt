You’ve used a pre-trained model to make predictions. That gives you great results if you want to classify images into the categories used by the original models. But what if you have a new use-case, and you don’t categorize images in exactly the same way as the categories for the pre-trained model?


For example, I might want a model that can tell if a photo was taken in an urban area or a rural area? My pre-trained model doesn’t classify images into those two specific categories.


We could build a new model from scratch for this specific purpose. But to get good results, we’d need thousands of photos with labels for which are urban and which are rural.


Something called transfer learning will give us good results with far less data. Transfer learning takes what a model learned while solving one problem, and applies it to a new application.


Remember that early layers of a deep learning model identify simple shapes. Later layers identify more complex visual patterns, and the very last layer makes predictions.


So most layers from a pre-trained are useful in new applications, because most computer vision problems involve similar low-level visual patterns.  So we’ll re-use most of the pre-trained ResNet model, and just replace the final layer that made predictions.


Some layers before that in the pretrained model may identify features like roads, buildings, windows, open fields, etc. We’ll drop in a replacement for the last layer of the ResNet model. This new last layer will predict whether an image is rural or urban based on the results of the previous layer.


Let’s look at this a little closer.


Here we see that the ResNet model has many layers.[x] We cut off the last layer.


The last layer of what’s left has information about our photo contents stored as a series of numbers in a tensor. It should be a 1 dimensional tensor, which is also called a vector.[y]


The vector can be shown as a series of dots. Each dot is called a node. The first node represents the first number in the vector. The second node represents the second number. And so on.  Practical models have more nodes than we’ve drawn here.[z]


We want to classify the images into two categories, urban and rural.  So after the last layer we keep of the pre-trained model, we add a new layer with two nodes. One node to capture how urban the photo is, and another to capture how rural it is.[aa]


In theory, any node in the last layer before prediction might inform how urban it is. So the urban measure can depend on all the nodes in this layer.  We draw connections to show that possible relationship[ab].  For the same reason, the information at each node might affect our measure of how rural the photo is.  So, our structure looks like this.


We have a lot of connections here, and we’ll use training data to determine which nodes suggest an image is urban, which suggest it is rural, and which don’t matter.  That is, we’re going to the training the last layer model. In practice, that training data will be photos that are labeled as being either rural or urban.  We’ll cover more mathematical detail on this training step in a later step.


Notice that we allow all features from one layer to influence or be connected with the prediction layer.  When this happens, we describe the last layer as being a dense layer.


One other note: When classifying something into only 2 categories, we could get by with only one node at the output.  In this case, a prediction for how urban a photo is would also be a measure of how rural it is.  If a photo is 80% likely to be urban, it would be 20% likely to be rural.


But we’ve kept two separate nodes at the output layer.  Using a separate node for each possible category in the output layer will help us transition to cases when we want to predict with more than 2 categories.


In both the current case and the case with more categories, we’ll get a score for each category, then apply a function called softmax. The softmax function will transform the scores to probabilities. So, they’ll all be positive, and they will sum to 1.[ac]


We could then work with those probabilities however we want.


Let’s see it in code.[ad]


We’ll introduce two new classes from Keras.  First is Sequential.  This is just saying we’re going to have a model that’s a sequence of layers, one after the other.  There are some exotic models that don’t fit aren’t this structure.  We’ll get to others types of models later. For now, all models you would want to build are Sequential. We’ll also want to add a Dense layer. So, we import that.[ae]


In this application, we classify photos into 2 categories or classes, urban and rural.  We’ll save that as num_classes.


Now we build the model. We set up a sequential model that we can add layers too. First we add all pretrained ResNet50 model.


We’ve written Include_top equals False. This is how we specify that we want to exclude the layer that makes predictions into the thousand categories used in the imagenet competition.  We’ll also use a file that doesn’t include the weights for that layer.


We have this argument pooling equals average.  That says that if we had extra channels in our tensor at the end of this step, we want to collapse them to a 1D tensor by taking an average.  We’ll come back to intricacies of pooling in a later lesson. But now we have a pretrained model that creates the layer you saw in the graphic.  We’ll add a Dense layer to make predictions.  We specify the number of nodes in this layer, which in this case is the number of classes, like we talked about earlier.  And then we say we want to apply the softmax function to turn it into probabilities.


We’ll tell tensorflow not to train the first layer, which is the ResNet50 model, because that’s the model that was already pre-trained with the imagenet data.  [af][ag]


Now we’ll get to a more complex line of code, the compile command.


I’ll describe the broad concept here, and we’ll give a more complete explanation of the underlying theory in a couple videos.  The compile command tells TensorFlow how to update the relationships in the Dense connections when we are doing the training with our data.


We have a measure of loss or inaccuracy we want to minimize. We specify it as categorical_crossentropy. In case you are familiar with logloss, this is another term for the same thing.


We use an algorithm called stochastic gradient descent to minimize the categorical_crossentropy loss function. Again, we’ll cover this in our theory video. We asked it to report the accuracy metric, that is what fraction of predictions were correct. This is easier to interpret than categorical crossentropy scores, so it’s nice to print it out and see how the model is doing.[ah]


Our raw data is broken into a directory of training data and a directory of validation data. Within each of those, we have one subdirectory for the urban pictures and another for the rural pictures. Keras provides a great tool for working with images grouped into directories by their label.[ai][aj]  This is the ImageDataGenerator.


There are two steps to using ImageDataGenerator. First we create the generator object in the abstract. I’ll tell it that we want to apply the Resnet preprocessing function every time it reads in an image. You used this function before, to be consistent with how the ResNet model was created.[ak]


Then we use this flow_from_directory command. We tell it what directory that data is in, what size image we want, how many images to read in at a time, and we tell it we’re classifying data into different categories.  We’ll add batch size to our list of information covered in the upcoming theory video.  For now, assume you’ll want categorical class mode almost every time.[al]


We do the same thing to set up a way to read the validation data.  That creates a validation_generator.


The ImageDataGenerator is especially very valuable when working with large datasets, because we don’t need to hold the whole dataset in memory at once. But it’s nice here, even with a small dataset.[am]


Now we fit the model.  We tell it the training data comes from train_generator.  We said to read 12 images at a time, and we have 72 training images. So we’ll go through six steps of twelve images.  Then we say the validation data comes from validation_generator. Validation generator read 20 images at a time, and we have 20 images of validation data.  So we can use 1 step.[an]


As the model training is running, we’ll see progress updates showing with our loss function and the accuracy. It updates the connections in the dense layer, that is the model’s impression of what makes an urban photo and what makes a rural photo, and it makes those updates in 6 steps. When it’s done, it got 79% of the training data right.  Then it examines the validation data. It gets 95% of those right. 19 out of 20.[ao]


I should mention that this is a really small dataset, and you should be hesitant about relying on validation scores from such a   small amount of data.  We’re starting with small datasets so you can get some experience under your belt with models that can be trained quickly.


But even with the small training dataset, this accuracy score is really good.
[ap]


We trained on 72 photos. You could easily take that many photos on your phone, upload them to Kaggle, and build a very accurate model to distinguish almost anything you care about. I think that’s pretty cool.


This may feel like a lot of new ideas for you to take in. Here’s the plan. We have one exercise for you to building a model yourself transfer learning.


After you’ve done transfer learning hands-on, I’ll show you a simple but powerful trick called data augmentation. Data augmentation really improves your computer vision models when working with small and medium sized datasets. That topic will be really quick.


Then I’ll come back and explain some of the theory that makes all of this possible.  That will clarify some loose ends and set the stage for you to build models from scratch.
